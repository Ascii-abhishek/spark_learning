{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import window, column, desc, col "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By default spark outputs 200 shuffle partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/25 21:30:27 WARN Utils: Your hostname, AS-MAC-0006.local resolves to a loopback address: 127.0.0.1; using fd01:db8:1111:0:0:0:0:3 instead (on interface lo0)\n",
      "23/04/25 21:30:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/25 21:30:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/25 21:30:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/04/25 21:30:29 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# spark = SparkSession.builder.appName(\"spark_definitive_guide\").config(\"spark.driver.host\", \"localhost\").getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"spark_definitive_guide\").config(\"spark.driver.host\", \"localhost\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data = \"./../src/flight-data/csv/2015-summary.csv\"\n",
    "flight_data_2015 = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(flight_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#19 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#19 ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [plan_id=55]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#17,ORIGIN_COUNTRY_NAME#18,count#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/abhishekpathak/Code & Stuff/self/Learning/SPARK/spark_lear..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_data_2015.sort(\"count\").explain()\n",
    "# flight_data_2015.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = \"./../src/retail-data/by-day/*\"\n",
    "df = spark.read.format('csv').option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.printSchema()\n",
    "df.createOrReplaceTempView(\"retail_data\")\n",
    "staticSchema = df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------------------------+------------------+\n",
      "|CustomerId|window                                    |sum(total_cost)   |\n",
      "+----------+------------------------------------------+------------------+\n",
      "|18287.0   |{2011-10-28 05:30:00, 2011-10-29 05:30:00}|70.67999999999999 |\n",
      "|18287.0   |{2011-05-22 05:30:00, 2011-05-23 05:30:00}|765.28            |\n",
      "|18287.0   |{2011-10-12 05:30:00, 2011-10-13 05:30:00}|1001.3199999999999|\n",
      "|18283.0   |{2011-11-10 05:30:00, 2011-11-11 05:30:00}|114.30000000000003|\n",
      "|18283.0   |{2011-10-27 05:30:00, 2011-10-28 05:30:00}|114.65            |\n",
      "|18283.0   |{2011-07-14 05:30:00, 2011-07-15 05:30:00}|143.19000000000008|\n",
      "|18283.0   |{2011-11-23 05:30:00, 2011-11-24 05:30:00}|313.6499999999997 |\n",
      "|18283.0   |{2011-06-14 05:30:00, 2011-06-15 05:30:00}|103.71999999999998|\n",
      "|18283.0   |{2011-12-06 05:30:00, 2011-12-07 05:30:00}|208.00000000000009|\n",
      "|18283.0   |{2011-04-21 05:30:00, 2011-04-22 05:30:00}|117.67999999999994|\n",
      "|18283.0   |{2011-02-28 05:30:00, 2011-03-01 05:30:00}|102.89999999999999|\n",
      "|18283.0   |{2011-05-23 05:30:00, 2011-05-24 05:30:00}|99.47             |\n",
      "|18283.0   |{2011-09-05 05:30:00, 2011-09-06 05:30:00}|134.89999999999998|\n",
      "|18283.0   |{2011-01-23 05:30:00, 2011-01-24 05:30:00}|106.55000000000001|\n",
      "|18283.0   |{2011-06-23 05:30:00, 2011-06-24 05:30:00}|203.8100000000001 |\n",
      "|18283.0   |{2011-11-30 05:30:00, 2011-12-01 05:30:00}|223.61000000000016|\n",
      "|18283.0   |{2011-01-06 05:30:00, 2011-01-07 05:30:00}|108.44999999999997|\n",
      "|18282.0   |{2011-12-02 05:30:00, 2011-12-03 05:30:00}|77.84             |\n",
      "|18282.0   |{2011-08-05 05:30:00, 2011-08-06 05:30:00}|100.21            |\n",
      "|18282.0   |{2011-08-09 05:30:00, 2011-08-10 05:30:00}|-1.45             |\n",
      "+----------+------------------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = df.selectExpr(\"CustomerId\", \"(UnitPrice * Quantity) as total_cost\", \"InvoiceDate\")\n",
    "# df1.show()\n",
    "df1 = df1.groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\")).sum(\"total_cost\")\n",
    "df1.sort(desc(\"CustomerId\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDF = spark.readStream.schema(staticSchema).option(\"maxFilesPerTrigger\", 1).format(\"csv\").option(\"header\", \"true\").load(path)\n",
    "\n",
    "purchaseByCustomerPerHour = streamingDF.selectExpr(\"CustomerId\", \"(UnitPrice * Quantity) as total_cost\", \"InvoiceDate\").groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\")).sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "def longestCommonPrefix(strs):\n",
    "        \n",
    "        s = sorted(strs, key=len)\n",
    "        if len(s) == 1 or len(s[0]) == 1:\n",
    "            return s[0]\n",
    "        print(s)\n",
    "        comm = \"\"\n",
    "        for i in range(len(s[0]) - 1):\n",
    "            print(i)\n",
    "            lett = s[0][i]\n",
    "            print(lett)\n",
    "            for j in s[1:]:\n",
    "                if j[i] != lett:\n",
    "                    print(j[i])\n",
    "                    return comm\n",
    "            comm = comm + lett\n",
    "        return comm\n",
    "\n",
    "print(longestCommonPrefix([\"ab\", \"a\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('spark_learning': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "221d6589af92ae1740d8ef81e7f5c4f5c2808ec9da299c9907514b6023ff7e8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
